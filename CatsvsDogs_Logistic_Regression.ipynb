{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cats vs. Dogs using Logistic Regression\n",
    "In this Jupyter Notebook, we'll implement the theory we learned in https://medium.com/@melodious/understanding-deep-neural-networks-from-first-principles-logistic-regression-bd2f01c9e263 by building a binary classifier that can predict whether an image contains a cat or a dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Import Packages\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The data is available at https://www.kaggle.com/c/dogs-vs-cats/data. Download and unzip train.zip. There are 25,000 images in the train.zip folder (12500 cat and 12500 dog images) - we'll use 80% (20k) for the training set and 20% (5k) for the test set. Move images numbered 1000 to 12499 from both dog and cat images into the ./datasets/test_cats_dogs folder. (Currently there are a few images in the folders ./datasets/train_cats_dogs and ./datasets/test_cats_dogs. Overwrite them with the dowloaded data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Preprocessing data in order to feed it into the model is often times the most time-consuming part of the machine learning process. In batch_processing() we resize the images to have a consistent width and height of 500 pixels. We then define a method load_data() to convert the images to a numpy array of size (width) x (height) x (# of channels). Since the files are named with either 'Cat' or 'Dog' in the filename, we check for the string and set the label y = 0 for a cat and y = 1 for a dog. We then randomize the files in the directory so that all the cats and dogs aren't lumped together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_preprocessing():\n",
    "    width = 500\n",
    "    height = 500\n",
    "    train_dir = 'datasets/train_cats_dogs/'\n",
    "    test_dir = 'datasets/test_cats_dogs/'\n",
    "\n",
    "    # Pre-process training set\n",
    "    for image in os.listdir(train_dir):\n",
    "        if image.endswith('.DS_Store'):\n",
    "            continue\n",
    "        print('Resizing image ' + image)\n",
    "        img = Image.open(os.path.join(train_dir,image))\n",
    "        img = img.resize((width,height), Image.BILINEAR)\n",
    "        img.save(os.path.join(train_dir,image))\n",
    "        \n",
    "    print('Batch pre-processing on {} files in training set complete'.format(len(os.listdir(train_dir))))\n",
    "    print('///////////////')\n",
    "    # Pre-process test set\n",
    "    for image in os.listdir(test_dir):\n",
    "        if image.endswith('.DS_Store'):\n",
    "            continue\n",
    "        print('Resizing image ' + image)\n",
    "        img = Image.open(os.path.join(test_dir,image))\n",
    "        img = img.resize((width,height), Image.BILINEAR)\n",
    "        img.save(os.path.join(test_dir,image))\n",
    "    print('Batch pre-processing on {} files in test set complete'.format(len(os.listdir(test_dir))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \n",
    "    train_dir = 'datasets/train_cats_dogs'\n",
    "    test_dir = 'datasets/test_cats_dogs'\n",
    "    \n",
    "    # initialize arrays\n",
    "    files_train = len(os.listdir(train_dir))\n",
    "    files_test = len(os.listdir(test_dir))\n",
    "    train_set_x = np.random.randint(0,255,size=(files_train,500,500,3),dtype='uint8')\n",
    "    train_set_y = np.zeros((1,files_train))\n",
    "    test_set_x = np.random.randint(0,255,size=(files_test,500,500,3),dtype='uint8')\n",
    "    test_set_y = np.zeros((1,files_test))\n",
    "\n",
    "    # load training files\n",
    "    for i,image in zip(range(files_train),os.listdir(train_dir)):   \n",
    "        if image.endswith('.DS_Store'):\n",
    "            continue\n",
    "        train_set_x[i] = np.array(Image.open(os.path.join(train_dir,image)))\n",
    "        # create training set labels (0: cat, 1: dog)\n",
    "        if 'dog' in str(image):\n",
    "            train_set_y[0,i] = 1\n",
    "            \n",
    "    # load test files \n",
    "    for i,image in zip(range(files_test),os.listdir(test_dir)):   \n",
    "        if image.endswith('.DS_Store'):\n",
    "            continue\n",
    "        test_set_x[i] = np.array(Image.open(os.path.join(test_dir,image)))\n",
    "        # create test set labels (0: cat, 1: dog)\n",
    "        if 'dog' in str(image):\n",
    "            test_set_y[0,i] = 1\n",
    "\n",
    "    # randomize files \n",
    "    idx_train = np.random.permutation(files_train)\n",
    "    idx_test = np.random.permutation(files_test)\n",
    "    train_set_x, train_set_y = train_set_x[idx_train,:], train_set_y[:,idx_train]\n",
    "    test_set_x, test_set_y = test_set_x[idx_test,:], test_set_y[:,idx_test]\n",
    "\n",
    "    # make sure arrays are correctly shaped\n",
    "    assert(train_set_x.shape == (files_train,500,500,3))\n",
    "    assert(train_set_y.shape == (1,files_train))\n",
    "    assert(test_set_x.shape == (files_test,500,500,3))\n",
    "    assert(test_set_y.shape == (1,files_test))\n",
    "    \n",
    "    return train_set_x,train_set_y,test_set_x,test_set_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize and split data\n",
    "After loading the data, it's helpful to visualize it.\n",
    "We also split the training data and keep 80% as the training set and 20% as the validation set. This way, we can train and check our results without contaminating and overfitting to our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_preprocessing()\n",
    "train_set_x,train_set_y,test_set_x,test_set_y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize an example\n",
    "plt.pyplot.imshow(train_set_x[19])\n",
    "print(train_set_y[0,19])\n",
    "\n",
    "# Split 80% of training set data, use other 20% as validation set\n",
    "cut_off = int(train_set_x.shape[0]*0.8)\n",
    "val = train_set_x.shape[0]-cut_off\n",
    "#print(cut_off)\n",
    "train_set_x_split = train_set_x[0:cut_off]\n",
    "train_set_y_split = train_set_y[0,0:cut_off].reshape(1,cut_off)\n",
    "validation_set_x = train_set_x[cut_off:]\n",
    "validation_set_y = train_set_y[0,cut_off:].reshape(1,val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten images\n",
    "An image is of shape 500 x 500 x 3 (height x width x number of channels). We'll flatten the matrix to be a vector of shape 500x500x3 = 750000 and standardize the image by dividing by 255 (the maximum value of a color pixel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of training, validation, and test examples and image dimensions  \n",
    "m_train = train_set_x.shape[0]\n",
    "m_val = validation_set_x.shape[0]\n",
    "m_test = test_set_x.shape[0]\n",
    "num_px = train_set_x.shape[1]\n",
    "\n",
    "# Flatten images into vectors\n",
    "train_set_x_flatten = train_set_x_split.reshape(train_set_x_split.shape[0],-1).T\n",
    "validation_set_x_flatten = np.reshape(validation_set_x,(num_px*num_px*3,m_val))\n",
    "test_set_x_flatten = np.reshape(test_set_x,(num_px*num_px*3,m_test))\n",
    "#print(train_set_x_flatten.shape,validation_set_x_flatten.shape, test_set_x_flatten.shape)\n",
    "\n",
    "# Standardize images by dividing by max value of a pixel channel (255)\n",
    "train_set_x_standardized = train_set_x_flatten/255.\n",
    "validation_set_x_standardized = validation_set_x_flatten/255.\n",
    "test_set_x_standardized = test_set_x_flatten/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print out all the shapes\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of validation examples: m_val = \" + str(m_val))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print(\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\")\n",
    "print (\"train_set_x shape: \" + str(train_set_x.shape))\n",
    "print (\"train_set_x_split shape: \" + str(train_set_x_split.shape))\n",
    "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
    "print (\"train_set_x_standardized shape: \" + str(train_set_x_standardized.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"train_set_y_split shape: \" + str(train_set_y_split.shape))\n",
    "print(\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\")\n",
    "print (\"validation_set_x shape: \" + str(validation_set_x.shape))\n",
    "print (\"validation_set_x_flatten shape: \" + str(validation_set_x_flatten.shape))\n",
    "print (\"validation_set_x_standardized shape: \" + str(validation_set_x_standardized.shape))\n",
    "print (\"validation_set_y shape: \" + str(validation_set_y.shape))\n",
    "print(\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\")\n",
    "print (\"test_set_x shape: \" + str(test_set_x.shape))\n",
    "print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
    "print (\"test_set_x_standardized shape: \" + str(test_set_x_standardized.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model\n",
    "\n",
    "We are finally ready to implement the model - we'll do so in steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-step\n",
    "First we define the sigmoid function. Then we implement forward and backward propagation. Note that we compute and store the gradients dw and db during backprop. We then use them to update w and b for each iteration during our optimization phase. Finally, we use the learned parameters to predict the value of the output, using 0.5 as a threshold value to determine whether y=0 or y=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z, deriv = False):\n",
    "    s = 1/(1+np.exp(-z))\n",
    "    if deriv:\n",
    "        ds = s*(1-s)\n",
    "        return s, ds\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def propagate(w,b,X,Y):\n",
    "    m = X.shape[1]\n",
    "    # forward prop\n",
    "    A = sigmoid(np.dot(np.transpose(w),X)+b)\n",
    "    cost = (-1/m)*np.sum(Y*np.log(A) + (1-Y)*np.log(1-A))\n",
    "    # backprop\n",
    "    dw = (1/m)*np.dot(X,np.transpose(A-Y))\n",
    "    db = (1/m)*np.sum(A-Y)\n",
    "\n",
    "    # Ensuring cost is a scalar by removing any dimensions of length 1\n",
    "    cost = np.squeeze(cost)\n",
    "\n",
    "    # Gradients are stored in a dictionary for use during optimization phase\n",
    "    grads = {\"dw\": dw,\n",
    "            \"db\": db}\n",
    "    \n",
    "    return grads,cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        grads, cost = propagate(w,b,X,Y)\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        w = w-learning_rate*dw\n",
    "        b = b-learning_rate*db\n",
    "        \n",
    "        #Record and print costs every 100 training examples\n",
    "        if i % 10 == 0:\n",
    "            costs.append(cost)    \n",
    "        if print_cost and i % 10 == 0:\n",
    "            print(\"Cost after iteration i: {}\".format(i,cost))\n",
    "    \n",
    "    # Params and grads are stored in a dictionary for use during each iteration\n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "        \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "        \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(w,b,X):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0],1)\n",
    "    A = sigmoid(np.dot(w.T,X)+b)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        Y_prediction[np.where(A>0.5)] = 1\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model\n",
    "We combine the helper functions defined above into a model. Note the values for the # of iterations and learning rate. Play around with those values to see how they affect the prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.6, print_cost=False):\n",
    "    \n",
    "    # Intialize w and b\n",
    "    w = np.zeros((X_train.shape[0],1))\n",
    "    b = np.zeros(1)\n",
    "    \n",
    "    # Learning the values of w and b \n",
    "    parameters, grads, costs = optimize(w,b,X_train,Y_train, num_iterations, learning_rate)\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Predicting the accuracy of the trained model on our test set \n",
    "    # We also predict how well the model does on the training set\n",
    "    Y_prediction_test = predict(w,b,X_test)\n",
    "    Y_prediction_train = predict(w,b,X_train)\n",
    "    \n",
    "    # Calculating the accuracy of the model\n",
    "    print(\"Training set accuracy: {}%\".format(100 - np.mean(np.abs(Y_prediction_train-Y_train) * 100)))\n",
    "    print(\"Test set accuracy: {}%\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    model_out = {\"costs\": costs,\n",
    "                     \"Y_prediction_test\": Y_prediction_test,\n",
    "                     \"Y_prediction_train\": Y_prediction_train,\n",
    "                     \"w\": w,\n",
    "                     \"b\": b,\n",
    "                     \"learning_rate\": learning_rate,\n",
    "                     \"num_iterations\": num_iterations}\n",
    "    return model_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the model on our dataset\n",
    "We've finally arrived at the moment of truth! How well does logistic regression do on the cats vs. dogs dataset?\n",
    "First we'll check train our model on the training set and check the accuracy on the validation set. See what learning rate and iterations give you the best accuracy value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.01, 0.005, 0.001]\n",
    "models = {}\n",
    "for i in learning_rates:\n",
    "    print (\"learning rate is: \" + str(i))\n",
    "    models[str(i)] = model(train_set_x_standardized, train_set_y_split, validation_set_x_standardized, validation_set_y, num_iterations = 1500, learning_rate = i, print_cost = False)\n",
    "    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n",
    "\n",
    "for i in learning_rates:\n",
    "    plt.pyplot.plot(np.squeeze(models[str(i)][\"costs\"]), label= str(models[str(i)][\"learning_rate\"]))\n",
    "\n",
    "plt.pyplot.ylabel('cost')\n",
    "plt.pyplot.xlabel('iterations')\n",
    "\n",
    "legend = plt.pyplot.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "plt.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll finally see how the model does on our test test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out = model(train_set_x_standardized, train_set_y_split, test_set_x_standardized, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve (with costs)\n",
    "costs = np.squeeze(model_out['costs'])\n",
    "plt.pyplot.plot(costs)\n",
    "plt.pyplot.ylabel('Cost')\n",
    "plt.pyplot.xlabel('Iterations (per hundreds)')\n",
    "plt.pyplot.title(\"Learning rate =\" + str(model_out[\"learning_rate\"]))\n",
    "plt.pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
